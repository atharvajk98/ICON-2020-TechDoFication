{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from queue import Queue\n",
    "import pandas as pd\n",
    "from indicnlp.tokenize.indic_tokenize import trivial_tokenize_indic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trivial_tokenize_indic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "        \n",
    "        # --------------------------------------- Constructor --------------------------------------- \n",
    "        \n",
    "        def __init__(self,stopword_list):\n",
    "            self.data_path = ''\n",
    "            self.stopword_list = stopword_list\n",
    "                \n",
    "\n",
    "        # --------------------------------------- Preprocess --------------------------------------- \n",
    "        \n",
    "        def expand_concatenations(self, word):\n",
    "            \n",
    "            \n",
    "            if not re.match('[a-zA-Z]+', word) or re.match('/d+',word):\n",
    "                for i in range(len(word)):\n",
    "                    if not('DEVANAGARI ' in unicodedata.name(word[i])):\n",
    "                        word = word[:i] if( len(word[i:]) < 2 and not word[i:].isnumeric()) else word[:i] + \" \" + word[i:]\n",
    "                        break\n",
    "            else:\n",
    "                for i in range(len(word)):\n",
    "                    if ('DEVANAGARI ' in unicodedata.name(word[i])):\n",
    "                        word = word[i:] if( len(word[:i]) < 2 and not word[:i].isnumeric() ) else word[:i] + \" \" + word[i:]\n",
    "                        break\n",
    "\n",
    "            return(word)\n",
    "    \n",
    "        \n",
    "        def clean_text(self,text: str) -> str:\n",
    "            try:\n",
    "                special_chars = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "                stemmer = PorterStemmer()\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                if not(isinstance(text, str)): text = str(text)\n",
    "\n",
    "                #Removing unprintable characters\n",
    "                text = ''.join(x for x in text if x.isprintable())\n",
    "\n",
    "                # Cleaning the urls\n",
    "                text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "                # Cleaning the html elements\n",
    "                text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "                # Removing the punctuations\n",
    "                text = re.sub('[!#?,.:\";-@#$%^&*_~<>()/\\-]', '', text)\n",
    "\n",
    "\n",
    "                # Removing stop words\n",
    "                text = ' '.join([word for word in text.split() if word not in self.stopword_list])\n",
    "\n",
    "                # Expanding noisy concatenations (Eg: algorithmआणि  -> algorithm आणि ) \n",
    "                text = ' '.join([self.expand_concatenations(word) for word in text.split()])\n",
    "\n",
    "#                 preprocessed_text = \"\"\n",
    "\n",
    "#                 for word in text.split(): \n",
    "#                     if (re.match('\\d+', word)):\n",
    "#                         if(word.isnumeric()):\n",
    "#                             preprocessed_text = preprocessed_text + '#N' + \" \"\n",
    "#                         else:\n",
    "#                             preprocessed_text = preprocessed_text + word.lower() + \" \"\n",
    "\n",
    "#                     else:\n",
    "#                         if(re.match('[a-zA-Z]+', word)):\n",
    "#                             if not len(word) < 2:\n",
    "#                                 word = word.lower()\n",
    "#     #                             word = lemmatizer.lemmatize(word, pos='v')\n",
    "#                                 preprocessed_text = preprocessed_text + word + \" \"\n",
    "\n",
    "#                         else:\n",
    "#                             preprocessed_text = preprocessed_text + word + \" \"\n",
    "\n",
    "#                 return preprocessed_text\n",
    "                return text\n",
    "            \n",
    "            except ValueError as ve:\n",
    "                print('Error processing:\\t',text)\n",
    "                return ''\n",
    "    \n",
    "        def preprocess_text(self,text: str) -> str:\n",
    "\n",
    "            try:\n",
    "                if not(isinstance(text, str)): text = str(text)\n",
    "                preprocessed_text = \"\"\n",
    "\n",
    "                for word in text.split(): \n",
    "                    if (re.match('\\d+', word)):\n",
    "                        if(word.isnumeric()):\n",
    "                            preprocessed_text = preprocessed_text + '#N' + \" \"\n",
    "                        else:\n",
    "                            preprocessed_text = preprocessed_text + word.lower() + \" \"\n",
    "\n",
    "                    else:\n",
    "                        if(re.match('[a-zA-Z]+', word)):\n",
    "                            if not len(word) < 2:\n",
    "                                word = word.lower()\n",
    "    #                             word = lemmatizer.lemmatize(word, pos='v')\n",
    "                                preprocessed_text = preprocessed_text + word + \" \"\n",
    "\n",
    "                        else:\n",
    "                            preprocessed_text = preprocessed_text + word + \" \"\n",
    "\n",
    "                return preprocessed_text\n",
    "\n",
    "            except ValueError as ve:\n",
    "#                 print('Error processing:\\t',text)\n",
    "                return ''\n",
    "            \n",
    "        def split_devanagri_word(self,word: str, punctuations = True) -> str:\n",
    "            try:\n",
    "                q = Queue()\n",
    "                l_index = 0\n",
    "                if not(isinstance(word, str)): word = str(word)\n",
    "                tokens = []\n",
    "                \n",
    "                for char in word:\n",
    "                    \n",
    "#                     print(char, '--->', unicodedata.name(char))\n",
    "                    if not 'devanagari' in unicodedata.name(char).lower():\n",
    "                        tokens.append(char)\n",
    "                        continue\n",
    "                        \n",
    "                    if 'letter' in unicodedata.name(char).lower():\n",
    "                        if q.empty():\n",
    "                            tokens.append(char)\n",
    "                        else:\n",
    "                            while not q.empty():\n",
    "                                tokens[len(tokens)-1] += q.get() \n",
    "                            tokens.append(char)   \n",
    "                    else:\n",
    "                        if punctuations == True:\n",
    "                            q.put(char)\n",
    "                    \n",
    "                for i, char in reversed(list(enumerate(tokens.copy()))):\n",
    "                    if('devanagari' in unicodedata.name(char).lower()):\n",
    "                        l_index = i\n",
    "#                         print(l_index)\n",
    "                        break\n",
    "                        \n",
    "                while not q.empty():\n",
    "                        tokens[l_index] += q.get() \n",
    "                \n",
    "                return tokens\n",
    "                \n",
    "            except Exception as e:\n",
    "#                 print('Error processing:\\t',word)\n",
    "                return ''\n",
    "        \n",
    "        def text2characters(self,text:str, punctuations = True)->str:\n",
    "            try:\n",
    "                if not(isinstance(text, str)): text = str(text)\n",
    "                char_sequence = \"\"\n",
    "                char_list = []\n",
    "                \n",
    "                for word in text.split():\n",
    "                    seq = ' '.join([char for char in self.split_devanagri_word(word, punctuations)])                \n",
    "                    char_sequence = char_sequence + seq + ' '\n",
    "    \n",
    "#                     print(word,'--->',seq)\n",
    "                    \n",
    "                return char_sequence\n",
    "            \n",
    "            except ValueError as ve:\n",
    "                print('Error processing:\\t',text)\n",
    "                return ''\n",
    "            \n",
    "            \n",
    "        def tokenize_characters(self, document):\n",
    "            vocab = set()\n",
    "            cnt = 0\n",
    "            token_dict = {}\n",
    "            \n",
    "            if isinstance(document, list):\n",
    "#                 print('Doc')\n",
    "                for text in document:\n",
    "                    char_sequence = self.text2characters(text)\n",
    "                    tokens_indic = pd.Series(trivial_tokenize_indic(char_sequence))\n",
    "                    word_counts = tokens_indic.value_counts()\n",
    "                    \n",
    "                    vocab = vocab.union(set(word_counts.keys()))\n",
    "\n",
    "                print('Total Unique Tokens (Characters): {}'.format(len(vocab)))\n",
    "\n",
    "                for char in vocab:\n",
    "                    cnt += 1\n",
    "                    token_dict[char] = cnt\n",
    "            \n",
    "            else:\n",
    "#                 print('sent')\n",
    "                char_sequence = self.text2characters(document)\n",
    "                tokens_indic = pd.Series(trivial_tokenize_indic(char_sequence))\n",
    "                word_counts = tokens_indic.value_counts()  \n",
    "                vocab = vocab.union(set(word_counts.keys()))\n",
    "\n",
    "                print('Total Unique Tokens (Characters): {}'.format(len(vocab)))\n",
    "\n",
    "                for char in vocab:\n",
    "                    cnt += 1\n",
    "                    token_dict[char] = cnt\n",
    "                \n",
    "            return token_dict\n",
    "\n",
    "        \n",
    "        def text_to_sequence(self,document,token_dict):\n",
    "            \n",
    "            sequence_doc = []\n",
    "            if isinstance(document, list):\n",
    "                print('Total records: ',len(document))\n",
    "                cnt = 0\n",
    "                for text in document:\n",
    "                    try:\n",
    "                        char_array = self.text2characters(text).split()\n",
    "                        text_sequence = [token_dict[x] for x in char_array]\n",
    "                        sequence_doc.append(text_sequence)\n",
    "                        cnt+=1\n",
    "                    except:\n",
    "                        print(text)\n",
    "                        \n",
    "                print('Records converted: ',cnt)\n",
    "                \n",
    "            else:\n",
    "                char_array = self.text2characters(document).split()\n",
    "                text_sequence = [token_dict[x] for x in char_array]\n",
    "                sequence_doc.append(text_sequence)\n",
    "                print('Records converted: 1')\n",
    "                \n",
    "            return sequence_doc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['तर , इंटरप्ट डिस्क्रिप्टर टेबलची सामग्री काय आहे ?']\n",
      "त्यांना CO2 2H20 सीओ२ लागेल99 , Animalत्यांना त्यांनाAnimal Analogy_त्यांना Science२१ १२Number \t--->\t त्यांना CO2  2H20 सीओ२ लागेल 99 Animal त्यांना त्यांना Animal Analogy त्यांना Science २१ १२ Number \n",
      "\n",
      "!@)$&%!#)&$!&$!$I am Atharva ११.२२ Kulkarni 11.22 a B 1 सी \t--->\t I am Atharva ११२२ Kulkarni  1122 a B  1 सी \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv('../Technodifacation/Data/training_data_marathi.csv')\n",
    "    \n",
    "    sampletext1 = df['text'].sample().values\n",
    "    print(sampletext1)\n",
    "    pp = Preprocess([])\n",
    "    sampletext2 = 'त्यांना जनतेला पटवून द्यावे लागेल99'\n",
    "\n",
    "    test_list1 = ['त्यांना','H20', '2H20','Animal2Animal' ,'सी२ओ२', 'लागेल99', 'Animalत्यांना',\n",
    "                 'त्यांनाAnimal', 'Analogy_त्यांना', 'Science२१', '१२Number', '!@)$&%!#)&$!&$!$B Bo ', '११.२२','I', '१','1','11.22','a','B','सी']\n",
    "\n",
    "    test_list2 = ['त्यांना CO2 2H20 सीओ२ लागेल99 , Animalत्यांना त्यांनाAnimal Analogy_त्यांना Science२१ १२Number',\n",
    "                 '!@)$&%!#)&$!&$!$I am Atharva ११.२२ Kulkarni 11.22 a B 1 सी']\n",
    "\n",
    "    for text in test_list2:\n",
    "        print(text, '\\t--->\\t', pp.clean_text(text),'\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocess([])\n",
    "from indicnlp.syllable import  syllabifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ज', 'ग', 'दी', 'श', 'चं', 'द्', 'र']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_word =  'जगदीशचंद्र'\n",
    "tokens = pp.split_devanagri_word(sample_word, punctuations=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text:  काही संस्थांना  1032017 लिहण्याची प्रथा आहे \n",
      "\n",
      "With Punctuations:  का ही सं स् थां ना 1 0 3 2 0 1 7 लि ह ण् या ची प् र था आ हे  \n",
      "\n",
      "Only Letters:  क ह स स थ न 1 0 3 2 0 1 7 ल ह ण य च प र थ आ ह \n"
     ]
    }
   ],
   "source": [
    "text = 'काही संस्थांना  10/3/2017 लिहण्याची प्रथा आहे'\n",
    "\n",
    "clean_text = pp.clean_text(text)\n",
    "\n",
    "char_sequence_1 = pp.text2characters(clean_text)\n",
    "char_sequence_2 = pp.text2characters(clean_text, punctuations=False)\n",
    "print('\\nText: ',clean_text,'\\n\\nWith Punctuations: ',char_sequence_1,'\\n\\nOnly Letters: ',char_sequence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>प्रा . प्रताप हरिदास : होय , मला वाटते की हा ए...</td>\n",
       "      <td>com_tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>तर , विशिष्ट गोष्टींद्वारे , ठराविक कायद्यांद्...</td>\n",
       "      <td>bioche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- - - - - - - - - - - - - - - - - - - - - - - ...</td>\n",
       "      <td>cse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>तर , आपला अर्धा चिन्ह 9 वाजता असेल .</td>\n",
       "      <td>phy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>म्हणून , मी असे म्हणालो की जर शेकडो , हजारो कि...</td>\n",
       "      <td>phy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41992</th>\n",
       "      <td>जरी आपण डेटा कूटबद्ध केला , तरीही हा मुख्य व्य...</td>\n",
       "      <td>cse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41993</th>\n",
       "      <td>ते म्हणतात - \" ज्याला पाहण्यासाठी डोळे , ऎकण्य...</td>\n",
       "      <td>com_tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41994</th>\n",
       "      <td>प्रथम क्रोनोलॉजिकल , क्रॉनोलॉजी म्हणजे आपल्याल...</td>\n",
       "      <td>com_tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>त्या थोड्या तपशीलावर येईल , जेणेकरून संपूर्ण ग...</td>\n",
       "      <td>bioche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>आणि एका हॉटेलमध्ये त्यांनी एका स्विस रेस्ट्राम...</td>\n",
       "      <td>com_tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text     label\n",
       "0      प्रा . प्रताप हरिदास : होय , मला वाटते की हा ए...  com_tech\n",
       "1      तर , विशिष्ट गोष्टींद्वारे , ठराविक कायद्यांद्...    bioche\n",
       "2      - - - - - - - - - - - - - - - - - - - - - - - ...       cse\n",
       "3                   तर , आपला अर्धा चिन्ह 9 वाजता असेल .       phy\n",
       "4      म्हणून , मी असे म्हणालो की जर शेकडो , हजारो कि...       phy\n",
       "...                                                  ...       ...\n",
       "41992  जरी आपण डेटा कूटबद्ध केला , तरीही हा मुख्य व्य...       cse\n",
       "41993  ते म्हणतात - \" ज्याला पाहण्यासाठी डोळे , ऎकण्य...  com_tech\n",
       "41994  प्रथम क्रोनोलॉजिकल , क्रॉनोलॉजी म्हणजे आपल्याल...  com_tech\n",
       "41995  त्या थोड्या तपशीलावर येईल , जेणेकरून संपूर्ण ग...    bioche\n",
       "41996  आणि एका हॉटेलमध्ये त्यांनी एका स्विस रेस्ट्राम...  com_tech\n",
       "\n",
       "[41997 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/original-dataset/marathi-training-data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text:  जरी आपण डेटा कूटबद्ध केला ani99. \n",
      "\n",
      "With Punctuations:  ज री आ प ण डे टा कू ट ब द् ध के ला a n i 9 9  \n",
      "\n",
      "Only Letters:  ज र आ प ण ड ट क ट ब द ध क ल a n i 9 9 \n"
     ]
    }
   ],
   "source": [
    "text = \"जरी आपण डेटा कूटबद्ध केला ani99.\"\n",
    "#clean_text = pp.clean_text(text)\n",
    "\n",
    "char_sequence_1 = pp.text2characters(pp.clean_text(text))\n",
    "char_sequence_2 = pp.text2characters(pp.clean_text(text), punctuations=False)\n",
    "print('\\nText: ',text,'\\n\\nWith Punctuations: ',char_sequence_1,'\\n\\nOnly Letters: ',char_sequence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idiotic Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('प', 11), ('्', 19), ('र', 14), ('ा', 26), ('त', 12), ('ह', 7), ('ि', 11), ('द', 5), ('स', 5), ('ो', 1), ('य', 8), ('म', 6), ('ल', 5), ('व', 11), ('ट', 3), ('े', 17), ('क', 14), ('ी', 6), ('ए', 1), ('च', 2), ('ु', 2), ('आ', 8), ('भ', 3), ('ं', 3), ('ळ', 1), ('ण', 5), ('श', 6), ('ष', 4), ('ः', 1), ('ै', 1), ('ू', 1), ('ज', 2), ('थ', 1), ('अ', 3), ('ग', 1), ('ध', 1), ('ठ', 1), ('ड', 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = False, split = \" \")\n",
    "tokenizer.fit_on_texts(char_sequence_1)\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Jugaad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आ     8\n",
      "क     7\n",
      "र     6\n",
      "प     5\n",
      "प्    5\n",
      "     ..\n",
      "री    1\n",
      "त     1\n",
      "रा    1\n",
      "ही    1\n",
      "हो    1\n",
      "Length: 67, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tokens_indic = trivial_tokenize_indic(char_sequence_1)\n",
    "\n",
    "tokens_indic = pd.Series(tokens_indic)\n",
    "\n",
    "word_counts = tokens_indic.value_counts()\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Devanagri Text to Char array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent\n",
      "Total Unique Tokens (Characters): 66\n",
      "Records converted: 1\n",
      "\n",
      " Text:\n",
      " हरिदास होय मला वाटते की हा एक महत्त्वाचा मुद्दा आहे की भारतीय संदर्भामुळे आपण विशेषतः आमच्या शैक्षणिक प्रक्रियेद्वारे प्रवेश करू शकता जिथे प्रवेश परीक्षा असते आणि जी आपल्याला विभागांमध्ये ठेवते आणि काही आपण त्या विशिष्ट क्रियाकलापांवर अडकले आहात असे कसे वाटते ----> 41\n",
      "\n",
      " Character array:\n",
      " ह रि दा स हो य म ला वा ट ते की हा ए क म ह त् त् वा चा मु द् दा आ हे की भा र ती य सं द र् भा मु ळे आ प ण वि शे ष तः आ म च् या शै क् ष णि क प् र क् रि ये द् वा रे प् र वे श क रू श क ता जि थे प् र वे श प री क् षा अ स ते आ णि जी आ प ल् या ला वि भा गां म ध् ये ठे व ते आ णि का ही आ प ण त् या वि शि ष् ट क् रि या क ला पां व र अ ड क ले आ हा त अ से क से वा ट ते  ----> 135\n",
      "\n",
      " Sequence array:\n",
      " [19, 52, 41, 24, 29, 49, 56, 18, 46, 5, 9, 45, 6, 43, 36, 56, 19, 7, 7, 46, 64, 42, 62, 41, 58, 27, 45, 47, 31, 65, 49, 35, 57, 30, 47, 42, 32, 58, 2, 16, 44, 4, 38, 48, 58, 56, 17, 40, 28, 11, 38, 61, 36, 14, 31, 11, 52, 8, 62, 46, 55, 14, 31, 54, 60, 36, 26, 60, 36, 53, 13, 12, 14, 31, 54, 60, 2, 21, 11, 10, 59, 24, 9, 58, 61, 15, 58, 2, 25, 40, 18, 44, 47, 34, 56, 1, 8, 37, 20, 9, 58, 61, 22, 23, 58, 2, 16, 7, 40, 44, 50, 51, 5, 11, 52, 40, 36, 18, 39, 20, 31, 59, 3, 36, 33, 58, 6, 63, 59, 66, 36, 66, 46, 5, 9] ----> 135\n"
     ]
    }
   ],
   "source": [
    "text = 'हरिदास होय मला वाटते की हा एक महत्त्वाचा मुद्दा आहे की भारतीय संदर्भामुळे आपण विशेषतः आमच्या शैक्षणिक प्रक्रियेद्वारे प्रवेश करू शकता जिथे प्रवेश परीक्षा असते आणि जी आपल्याला विभागांमध्ये ठेवते आणि काही आपण त्या विशिष्ट क्रियाकलापांवर अडकले आहात असे कसे वाटते'\n",
    "clean_text = pp.clean_text(text)\n",
    "\n",
    "token_dict = pp.tokenize_characters(clean_text)\n",
    "text_seq = pp.text_to_sequence(clean_text, token_dict)\n",
    "\n",
    "print('\\n Text:\\n',clean_text,'---->' ,len((clean_text).split()))\n",
    "print('\\n Character array:\\n',pp.text2characters(clean_text),'---->' ,len(pp.text2characters(clean_text).split()))\n",
    "print('\\n Sequence array:\\n',text_seq[0],'---->',len(text_seq[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41997"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------Trial on original dataset-------------------------------------#\n",
    "\n",
    "df = pd.read_csv('../Technodifacation/Data/training_data_marathi.csv')\n",
    "x_train = df['text'].apply(lambda x : pp.clean_text(x)).tolist()\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc\n",
      "Total Unique Tokens (Characters): 790\n",
      "Total records:  41997\n",
      "Records converted:  41997\n"
     ]
    }
   ],
   "source": [
    "# Two step conversion\n",
    "\n",
    "token_dict = pp.tokenize_characters(x_train)\n",
    "x_train_tokenized = pp.text_to_sequence(x_train, token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \n",
      " किमान  10  12 आइस क्रीम तेथे आहेत आणि अशा प्रकारे व्यवस्था केली जाते की किमान  8 ते  10 लॉली एका ठिकाणी ठेवल्या जाऊ शकतात \n",
      "Num Words: 24 \n",
      "\n",
      " Sequence: \n",
      " [449, 354, 500, 723, 283, 723, 25, 486, 309, 119, 17, 420, 673, 108, 506, 486, 446, 91, 486, 683, 393, 52, 600, 141, 311, 382, 209, 60, 113, 568, 665, 389, 99, 607, 108, 159, 449, 354, 500, 407, 108, 723, 283, 249, 99, 352, 311, 174, 311, 104, 340, 113, 322, 153, 607, 271, 583, 631, 669, 91]\n",
      " Sequence len: 60\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i = random.randint(0,len(x_train))\n",
    "print('Text: \\n {} \\nNum Words: {} \\n\\n Sequence: \\n {}\\n Sequence len: {}'.format(x_train[i],len(x_train[i].split()),\n",
    "                                                                    x_train_tokenized[i],len(x_train_tokenized[i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
