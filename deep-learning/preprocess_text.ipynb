{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# from inltk.inltk import tokenize\n",
    "# from inltk.inltk import identify_language\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "import pandas as pd\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \n",
    "        # --------------------------------------- Constructor --------------------------------------- \n",
    "        \n",
    "        def __init__(self,stopword_list):\n",
    "            self.data_path = ''\n",
    "            self.stopword_list = stopword_list\n",
    "            \n",
    "    \n",
    "        # --------------------------------------- Preprocess --------------------------------------- \n",
    "        \n",
    "        def clean_text(self,text):\n",
    "            \n",
    "            special_chars = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "            text = str(text)\n",
    "            # Lemmatizing English words\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            stemmer = PorterStemmer() \n",
    "            \n",
    "            # Cleaning the urls\n",
    "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "            # Cleaning the html elements\n",
    "            text = re.sub(r'<.*?>', '', text)\n",
    "            \n",
    "            #Cleaning Special characters\n",
    "\n",
    "            \n",
    "            # Removing the punctuations\n",
    "            text = re.sub('[!#?,.:\";-@#$%^&*_~<>()-]', ' ', text)\n",
    "                    \n",
    "            # Removing stop words\n",
    "            text = ' '.join([word for word in text.split() if word not in self.stopword_list])\n",
    "            \n",
    "            # Cleaning the whitespaces\n",
    "#             text = re.sub(r'\\s+', '', text).strip()\n",
    "\n",
    "            preprocessed_text = \"\"\n",
    "            \n",
    "            for word in text.split(): \n",
    "#                 if any(chr.isalpha() for chr in word) and any(chr.isdigit() for chr in word): \n",
    "                    if (re.match('\\d+', word)):\n",
    "                       if(word.isnumeric()):\n",
    "                           preprocessed_text = preprocessed_text + '<Numeric>' + \" \"\n",
    "                    \n",
    "                    else:\n",
    "                            if re.match('[a-zA-Z]+', word):\n",
    "                                word = word.lower()\n",
    "                                word = lemmatizer.lemmatize(word)\n",
    "                                preprocessed_text = preprocessed_text + word + \" \"\n",
    "                                \n",
    "                            else:\n",
    "                                preprocessed_text = preprocessed_text + word + \" \"\n",
    "                       \n",
    "\n",
    "            # Lemmatizing Marathi words (optional)\n",
    "#             for word in text:\n",
    "#                 if not re.match('[a-zA-Z]+',word) and word != '#s':\n",
    "                    \n",
    "            \n",
    "            \n",
    "            return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before:\t !@#@$ ! $!@ $!@$ !@$ ! $ एका विशिष्ट-- 19022323239  great 2T2 ,H2O, 9909च, Having Caring Sharing शब्दाचा उच्चार कसा केला गेला आणि 99 Working समन्वय साधण्याचा प्रयत्न करा जेव्हा 87929999 एका बिंदूबरोबर इतर गोष्टींचा विचार केला तर आपण एका चांगल्या स्थितीत जाऊ शकता!!! ११000 १ Google computer architecture graphic show.!!! \n",
      "\n",
      "After:\t एका विशिष्ट <Numeric> great h2o having caring sharing शब्दाचा उच्चार कसा केला गेला आणि <Numeric> working समन्वय साधण्याचा प्रयत्न करा जेव्हा <Numeric> एका बिंदूबरोबर इतर गोष्टींचा विचार केला तर आपण एका चांगल्या स्थितीत जाऊ शकता <Numeric> <Numeric> google computer architecture graphic show \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   \n",
    "    df = pd.read_csv('../Technodifacation/Data/training_data_marathi.csv')\n",
    "    stopword_list = []\n",
    "    \n",
    "#     with open ('../Technodifacation/Data/marathi_stopwords.txt','r',encoding='utf') as st:\n",
    "#         st_content = st.read()\n",
    "#         st_list = set(st_content.split())\n",
    "#         stopword_list = st_list\n",
    "    \n",
    "    pp = Preprocess([])\n",
    "    df['text'] = df['text'].apply(lambda x : pp.clean_text(x))\n",
    "#     sample_text = df.sample()['text'].values[0]\n",
    "    sample_text = \"!@#@$ ! $!@ $!@$ !@$ ! $ एका विशिष्ट-- 19022323239  great 2T2 ,H2O, 9909च, Having Caring Sharing शब्दाचा उच्चार कसा केला गेला आणि 99 Working समन्वय साधण्याचा प्रयत्न करा जेव्हा 87929999 एका बिंदूबरोबर इतर गोष्टींचा विचार केला तर आपण एका चांगल्या स्थितीत जाऊ शकता!!! ११000 १ Google computer architecture graphic show.!!!\"\n",
    "    preprocessed_text = pp.clean_text(sample_text)\n",
    "    print('\\nBefore:\\t',sample_text,'\\n\\nAfter:\\t',preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vocab(vocab_words):\n",
    "    numbers = []\n",
    "    english_words = []\n",
    "    marathi_words = []\n",
    "    for word in vocab_words:\n",
    "        if re.match('\\d+', word):\n",
    "            numbers.append(word)\n",
    "        elif re.match('[a-zA-Z]+', word):\n",
    "            english_words.append(word)\n",
    "        else:\n",
    "            marathi_words.append(word)\n",
    "    return numbers, english_words, marathi_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51955 128\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('../Technodifacation/Data/training_data_marathi.csv')\n",
    "\n",
    "pp = Preprocess([])\n",
    "text = df['text'].apply(lambda x : pp.clean_text(x)).to_list()\n",
    "\n",
    "alpha_numeric = set()\n",
    "vocab = set()\n",
    "\n",
    "for t in text:\n",
    "        for word in t.split(): \n",
    "            word = word.lower()\n",
    "            vocab.add(word)\n",
    "            if any(chr.isalpha() for chr in word) and any(chr.isdigit() for chr in word): \n",
    "#                 print(word)\n",
    "                alpha_numeric.add(word)\n",
    "        \n",
    "print(len(vocab),len(alpha_numeric))\n",
    "\n",
    "num , en , mar = analyze_vocab(vocab)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
