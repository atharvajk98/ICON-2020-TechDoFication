{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from textblob import TextBlob, Word\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "        \n",
    "        # --------------------------------------- Constructor --------------------------------------- \n",
    "        \n",
    "        def __init__(self,stopword_list):\n",
    "            self.data_path = ''\n",
    "            self.stopword_list = stopword_list\n",
    "            \n",
    "    \n",
    "        # --------------------------------------- Preprocess --------------------------------------- \n",
    "        \n",
    "        def expand_concatenations(self, word):\n",
    "            \n",
    "            if not re.match('[a-zA-Z]+', word) or re.match('/d+',word):\n",
    "                for i in range(len(word)):\n",
    "                    if not('DEVANAGARI ' in unicodedata.name(word[i])):\n",
    "                        word = word[:i] if( len(word[i:]) < 2 ) else word[:i] + \" \" + word[i:]\n",
    "                        break\n",
    "            else:\n",
    "                for i in range(len(word)):\n",
    "                    if ('DEVANAGARI ' in unicodedata.name(word[i])):\n",
    "                        word = word[i:] if( len(word[:i]) < 2 ) else word[:i] + \" \" + word[i:]\n",
    "                        break\n",
    "\n",
    "            return(word)\n",
    "    \n",
    "        \n",
    "        \n",
    "        def clean_text(self,text):\n",
    "            \n",
    "            special_chars = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "            stemmer = PorterStemmer()\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            \n",
    "            text = str(text)\n",
    "\n",
    "            # Cleaning the urls\n",
    "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "            # Cleaning the html elements\n",
    "            text = re.sub(r'<.*?>', '', text)\n",
    "            \n",
    "            # Removing the punctuations\n",
    "            text = re.sub('[!#?,.:\";-@#$%^&*_~<>()-]', ' ', text)\n",
    "                    \n",
    "            # Removing stop words\n",
    "            text = ' '.join([word for word in text.split() if word not in self.stopword_list])\n",
    "            \n",
    "            # Expanding noisy concatenations (Eg: algorithmआणि  -> algorithm आणि ) \n",
    "            text = ' '.join([self.expand_concatenations(word) for word in text.split()])\n",
    "            \n",
    "            preprocessed_text = \"\"\n",
    "            \n",
    "            for word in text.split(): \n",
    "                if (re.match('\\d+', word)):\n",
    "                    if(word.isnumeric()):\n",
    "                        preprocessed_text = preprocessed_text + '#N' + \" \"\n",
    "\n",
    "                else:\n",
    "                    if(re.match('[a-zA-Z]+', word) and len(word) > 1):\n",
    "                            word = word.lower()\n",
    "#                             word = lemmatizer.lemmatize(word, pos='v')\n",
    "                            preprocessed_text = preprocessed_text + word + \" \"\n",
    "\n",
    "                    else:\n",
    "                        preprocessed_text = preprocessed_text + word + \" \"\n",
    "            \n",
    "            return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vocab(vocab_words):\n",
    "    numbers = []\n",
    "    english_words = []\n",
    "    marathi_words = []\n",
    "    for word in vocab_words:\n",
    "        if re.match('\\d+', word):\n",
    "            numbers.append(word)\n",
    "        elif re.match('[a-zA-Z]+', word):\n",
    "            english_words.append(word)\n",
    "        else:\n",
    "            marathi_words.append(word)\n",
    "    return numbers, english_words, marathi_words\n",
    "\n",
    "def custom_analyzer(text):\n",
    "    # extract words of at least 1 letters\n",
    "    words = re.findall(r'\\w{1,}', text)\n",
    "    for w in words:\n",
    "        yield w\n",
    "\n",
    "def bow_vectorize(x_train, x_val):\n",
    "        bow_vectorizer = CountVectorizer(analyzer=custom_analyzer)\n",
    "        bow_vectorizer.fit(x_train)\n",
    "        bow_x_train = bow_vectorizer.transform(x_train)\n",
    "        bow_x_val = bow_vectorizer.transform(x_val)\n",
    "        return bow_vectorizer, bow_x_train, bow_x_val\n",
    "\n",
    "def check_alphanumeric_words(text):\n",
    "    alpha_numeric_set = set()\n",
    "    for t in text:\n",
    "        for word in t.split(): \n",
    "            if any(chr.isalpha() for chr in word) and any(chr.isdigit() for chr in word): \n",
    "#                 print(word)\n",
    "                alpha_numeric_set.add(word)\n",
    "    return alpha_numeric_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('../Technodifacation/Data/training_data_marathi.csv')\n",
    "\n",
    "pp = Preprocess([])\n",
    "text = df['text'].apply(lambda x : pp.clean_text(x)).tolist()\n",
    "alpha_numeric = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim tokenizer: Unique tokens = 51915 , Alphanumeric = 96\n",
      "#Num = 0 , #Eng = 937, #Mar = 50978 \n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "word_index = tokenizer.word_index\n",
    "        \n",
    "print('Gensim tokenizer: Unique tokens = {} , Alphanumeric = {}'.format(len(word_index),len(check_alphanumeric_words(word_index))))\n",
    "\n",
    "num , en1 , mar1 = analyze_vocab(word_index)\n",
    "print('#Num = {} , #Eng = {}, #Mar = {} '.format(len(num),len(en1),len(mar1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim tokenizer: Unique tokens = 51942 , Alphanumeric = 96\n",
      "#Num = 0 , #Eng = 961, #Mar = 50981 \n"
     ]
    }
   ],
   "source": [
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "indic_nlp_tokens = set()\n",
    "for record in text:\n",
    "    tokens = indic_tokenize.trivial_tokenize(record, lang = 'mar')\n",
    "    for t in tokens:\n",
    "        indic_nlp_tokens.add(t)\n",
    "\n",
    "\n",
    "print('Gensim tokenizer: Unique tokens = {} , Alphanumeric = {}'.format(len(indic_nlp_tokens),len(check_alphanumeric_words(indic_nlp_tokens))))\n",
    "\n",
    "num , en2 , mar2 = analyze_vocab(indic_nlp_tokens)\n",
    "print('#Num = {} , #Eng = {}, #Mar = {} '.format(len(num),len(en2),len(mar2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra words tokenized by Indic - NLP: ['D', 'S', 'V', 'L', 'F', 'H', 'A', 'Z', 'R', 'B', 'W', 'O', 'X', 'N', 'I', 'P', 'Y', 'G', 'U', 'K', 'C', 'M', 'T', 'E'] ['', '°C', 'σD', '`', '#']\n"
     ]
    }
   ],
   "source": [
    "ex1 = [x for x in en2 if x not in en1]\n",
    "ex2 = [x for x in mar2 if x not in mar1]\n",
    "\n",
    "print('Extra words tokenized by Indic - NLP:',ex1,ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41997, 5501)\n",
      "(3780, 5501)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "training_data = pd.read_csv(\"../Technodifacation/Data/training_data_marathi.csv\")\n",
    "training_data['text'] = training_data.text.apply(lambda x: pp.clean_text(x))\n",
    "x_train = training_data.text.values.tolist()\n",
    "val_data = pd.read_csv(\"../Technodifacation/Data/test_data_marathi.csv\")\n",
    "val_data['text'] = val_data.text.apply(lambda x: pp.clean_text(x))\n",
    "x_val = val_data.text.values.tolist()\n",
    "\n",
    "bow_vectorizer, bow_x_train, bow_x_val = bow_vectorize(x_train, x_val)\n",
    "print(bow_x_train.shape)\n",
    "print(bow_x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before:\t mस्केलेबल algorithmआणि  एमएनओMOM आयपीIPs  वारेB ०००० किलोमीट विशिष्ट-- 19022323239  ० great 2T2 ,H2O, 9909च, Having Caring Sharing शब्दाचा उच्चार कसा केला गेला आणि 99 Working समन्वय साधण्याचा प्रयत्न करा जेव्हा 87929999 एका बिंदूबरोबर इतर गोष्टींचा एका!!! ११ 00 १ Google computer architecture graphic show.!!! \n",
      "\n",
      "After:\t स्केलेबल algorithm आणि एमएनओ mom आयपी ips वारे #N किलोमीट विशिष्ट #N #N great h2o having caring sharing शब्दाचा उच्चार कसा केला गेला आणि #N working समन्वय साधण्याचा प्रयत्न करा जेव्हा #N एका बिंदूबरोबर इतर गोष्टींचा एका #N #N #N google computer architecture graphic show \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   \n",
    "    df = pd.read_csv('../Technodifacation/Data/training_data_marathi.csv')\n",
    "    stopword_list = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#     with open ('../Technodifacation/Data/marathi_stopwords.txt','r',encoding='utf') as st:\n",
    "#         st_content = st.read()\n",
    "#         st_list = set(st_content.split())\n",
    "#         stopword_list = st_list\n",
    "    \n",
    "    pp = Preprocess([])\n",
    "    \n",
    "#     df['text'] = df['text'].apply(lambda x : pp.clean_text(x))\n",
    "#     sample_text = df.sample()['text'].values[0]\n",
    "\n",
    "    sample_text = \"mस्केलेबल algorithmआणि  एमएनओMOM आयपीIPs  वारेB ०००० किलोमीट विशिष्ट-- 19022323239  ० great 2T2 ,H2O, 9909च, Having Caring Sharing शब्दाचा उच्चार कसा केला गेला आणि 99 Working समन्वय साधण्याचा प्रयत्न करा जेव्हा 87929999 एका बिंदूबरोबर इतर गोष्टींचा एका!!! ११ 00 १ Google computer architecture graphic show.!!!\"\n",
    "    preprocessed_text = pp.clean_text(sample_text)\n",
    "    print('\\nBefore:\\t',sample_text,'\\n\\nAfter:\\t',preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
